{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# https://gym.openai.com/evaluations/eval_lEi8I8v2QLqEgzBxcvRIaA\n",
    "\n",
    "\"\"\" Quick script for an \"Episodic Controller\" Agent, i.e. nearest neighbor \"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "import gym, sys\n",
    "\n",
    "class EpisodicAgent(object):\n",
    "    \"\"\"\n",
    "    Episodic agent is a simple nearest-neighbor based agent:\n",
    "    - At training time it remembers all tuples of (state, action, reward).\n",
    "    - After each episode it computes the empirical value function based \n",
    "        on the recorded rewards in the episode.\n",
    "    - At test time it looks up k-nearest neighbors in the state space \n",
    "        and takes the action that most often leads to highest average value.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space):\n",
    "        self.action_space = action_space\n",
    "        assert isinstance(action_space, gym.spaces.discrete.Discrete), 'unsupported action space for now.'\n",
    "\n",
    "        # options\n",
    "        self.epsilon = 1.0 # probability of choosing a random action\n",
    "        self.epsilon_decay = 0.98 # decay of epsilon per episode\n",
    "        self.epsilon_min = 0\n",
    "        self.nnfind = 500 # how many nearest neighbors to consider in the policy?\n",
    "        self.mem_needed = 500 # amount of data to have before we can start exploiting\n",
    "        self.mem_size = 50000 # maximum size of memory\n",
    "        self.gamma = 0.95 # discount factor\n",
    "\n",
    "        # internal vars\n",
    "        self.iter = 0\n",
    "        self.mem_pointer = 0 # memory pointer\n",
    "        self.max_pointer = 0\n",
    "        self.db = None # large array of states seen\n",
    "        self.dba = {} # actions taken\n",
    "        self.dbr = {} # rewards obtained at all steps\n",
    "        self.dbv = {} # value function at all steps, computed retrospectively\n",
    "        self.ep_start_pointer = 0\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        assert isinstance(observation, np.ndarray) and observation.ndim == 1, 'unsupported observation type for now.'\n",
    "\n",
    "        if self.db is None:\n",
    "            # lazy initialization of memory\n",
    "            self.db = np.zeros((self.mem_size, observation.size))\n",
    "            self.mem_pointer = 0\n",
    "            self.ep_start_pointer = 0\n",
    "\n",
    "        # we have enough data, we want to explore, and we have seen at least one episode already (so values were computed)\n",
    "        if self.iter > self.mem_needed and np.random.rand() > self.epsilon and self.dbv:\n",
    "            # exploit: find the few closest states and pick the action that led to highest rewards\n",
    "            # 1. find k nearest neighbors\n",
    "            ds = np.sum((self.db[:self.max_pointer] - observation)**2, axis=1) # L2 distance\n",
    "            ix = np.argsort(ds) # sorts ascending by distance\n",
    "            # ix = ix[:min(len(ix), self.nnfind)] # crop to only some number of nearest neighbors\n",
    "            ix = ix[:min(len(ix), 1000)]\n",
    "            \n",
    "            # find the action that leads to most success. do a vote among actions\n",
    "            adict = {}\n",
    "            ndict = {}\n",
    "            for i in ix:\n",
    "                vv = self.dbv[i]\n",
    "                aa = self.dba[i]\n",
    "                vnew = adict.get(aa, 0) + vv\n",
    "                adict[aa] = vnew                          #adict[1] = somevalue, adict[0] = somevalue\n",
    "                ndict[aa] = ndict.get(aa, 0) + 1          #keeps a count of the number of times an action was taken\n",
    "\n",
    "            for a in adict: # normalize by counts\n",
    "                adict[a] = adict[a] / ndict[a]\n",
    "            # print ('Action Dict:',adict)\n",
    "            \n",
    "            its = [(y,x) for x,y in adict.items()]   #sort([value, action for action, value in adict.items()])\n",
    "            its.sort(reverse=True) # descending\n",
    "            a = its[0][1]\n",
    "\n",
    "        else:\n",
    "            # explore: do something random\n",
    "            a = self.action_space.sample()\n",
    "\n",
    "        # record move to database\n",
    "        if self.mem_pointer < self.mem_size:\n",
    "            self.db[self.mem_pointer] = observation # save the state\n",
    "            self.dba[self.mem_pointer] = a # and the action we took\n",
    "            self.dbr[self.mem_pointer-1] = reward # and the reward we obtained last time step\n",
    "            self.dbv[self.mem_pointer-1] = 0\n",
    "        self.mem_pointer += 1\n",
    "        self.iter += 1\n",
    "\n",
    "        if done: # episode Ended;\n",
    "\n",
    "            # compute the estimate of the value function based on this rollout\n",
    "            v = 0\n",
    "            for t in reversed(range(self.ep_start_pointer, self.mem_pointer)):\n",
    "                v = self.gamma * v + self.dbr.get(t,0)\n",
    "                self.dbv[t] = v\n",
    "\n",
    "            self.ep_start_pointer = self.mem_pointer\n",
    "            self.max_pointer = min(max(self.max_pointer, self.mem_pointer), self.mem_size)\n",
    "            \n",
    "            # decay exploration probability\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "            self.epsilon = max(self.epsilon, self.epsilon_min) # cap at epsilon_min\n",
    "\n",
    "            # print ('===============memory size: ', self.mem_pointer)\n",
    "\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-25 13:57:15,821] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 running reward: 1.900000 actual reward: 38.000000\n",
      "===============memory size:  39\n",
      "1 running reward: 3.955000 actual reward: 43.000000\n",
      "===============memory size:  82\n",
      "2 running reward: 5.357250 actual reward: 32.000000\n",
      "===============memory size:  114\n",
      "3 running reward: 6.139387 actual reward: 21.000000\n",
      "===============memory size:  135\n",
      "4 running reward: 6.732418 actual reward: 18.000000\n",
      "===============memory size:  153\n",
      "5 running reward: 7.195797 actual reward: 16.000000\n",
      "===============memory size:  169\n",
      "6 running reward: 8.486007 actual reward: 33.000000\n",
      "===============memory size:  202\n",
      "7 running reward: 8.561707 actual reward: 10.000000\n",
      "===============memory size:  212\n",
      "8 running reward: 9.583622 actual reward: 29.000000\n",
      "===============memory size:  241\n",
      "9 running reward: 9.654441 actual reward: 11.000000\n",
      "===============memory size:  252\n",
      "10 running reward: 12.271719 actual reward: 62.000000\n",
      "===============memory size:  314\n",
      "11 running reward: 12.408133 actual reward: 15.000000\n",
      "===============memory size:  329\n",
      "12 running reward: 12.787726 actual reward: 20.000000\n",
      "===============memory size:  349\n",
      "13 running reward: 13.698340 actual reward: 31.000000\n",
      "===============memory size:  380\n",
      "14 running reward: 13.963423 actual reward: 19.000000\n",
      "===============memory size:  399\n",
      "15 running reward: 14.265252 actual reward: 20.000000\n",
      "===============memory size:  419\n",
      "16 running reward: 15.151989 actual reward: 32.000000\n",
      "===============memory size:  451\n",
      "17 running reward: 15.244390 actual reward: 17.000000\n",
      "===============memory size:  468\n",
      "18 running reward: 15.482170 actual reward: 20.000000\n",
      "===============memory size:  488\n",
      "19 running reward: 16.058062 actual reward: 27.000000\n",
      "===============memory size:  515\n",
      "20 running reward: 15.905158 actual reward: 13.000000\n",
      "===============memory size:  528\n",
      "21 running reward: 15.659901 actual reward: 11.000000\n",
      "===============memory size:  539\n",
      "22 running reward: 15.376906 actual reward: 10.000000\n",
      "===============memory size:  549\n",
      "23 running reward: 15.808060 actual reward: 24.000000\n",
      "===============memory size:  573\n",
      "24 running reward: 15.867657 actual reward: 17.000000\n",
      "===============memory size:  590\n",
      "25 running reward: 15.624274 actual reward: 11.000000\n",
      "===============memory size:  601\n",
      "26 running reward: 15.443061 actual reward: 12.000000\n",
      "===============memory size:  613\n",
      "27 running reward: 15.270908 actual reward: 12.000000\n",
      "===============memory size:  625\n",
      "28 running reward: 16.107362 actual reward: 32.000000\n",
      "===============memory size:  657\n",
      "29 running reward: 15.951994 actual reward: 13.000000\n",
      "===============memory size:  670\n",
      "30 running reward: 15.904394 actual reward: 15.000000\n",
      "===============memory size:  685\n",
      "31 running reward: 15.709175 actual reward: 12.000000\n",
      "===============memory size:  697\n",
      "32 running reward: 15.873716 actual reward: 19.000000\n",
      "===============memory size:  716\n",
      "33 running reward: 15.730030 actual reward: 13.000000\n",
      "===============memory size:  729\n",
      "34 running reward: 15.593529 actual reward: 13.000000\n",
      "===============memory size:  742\n",
      "35 running reward: 15.363852 actual reward: 11.000000\n",
      "===============memory size:  753\n",
      "36 running reward: 15.495660 actual reward: 18.000000\n",
      "===============memory size:  771\n",
      "37 running reward: 15.120877 actual reward: 8.000000\n",
      "===============memory size:  779\n",
      "38 running reward: 15.064833 actual reward: 14.000000\n",
      "===============memory size:  793\n",
      "39 running reward: 14.961591 actual reward: 13.000000\n",
      "===============memory size:  806\n",
      "40 running reward: 14.763512 actual reward: 11.000000\n",
      "===============memory size:  817\n",
      "41 running reward: 14.825336 actual reward: 16.000000\n",
      "===============memory size:  833\n",
      "42 running reward: 14.684069 actual reward: 12.000000\n",
      "===============memory size:  845\n",
      "43 running reward: 15.099866 actual reward: 23.000000\n",
      "===============memory size:  868\n",
      "44 running reward: 15.044872 actual reward: 14.000000\n",
      "===============memory size:  882\n",
      "45 running reward: 15.042629 actual reward: 15.000000\n",
      "===============memory size:  897\n",
      "46 running reward: 14.840497 actual reward: 11.000000\n",
      "===============memory size:  908\n",
      "47 running reward: 14.648473 actual reward: 11.000000\n",
      "===============memory size:  919\n",
      "48 running reward: 14.716049 actual reward: 16.000000\n",
      "===============memory size:  935\n",
      "49 running reward: 14.680246 actual reward: 14.000000\n",
      "===============memory size:  949\n",
      "50 running reward: 14.446234 actual reward: 10.000000\n",
      "===============memory size:  959\n",
      "51 running reward: 14.273922 actual reward: 11.000000\n",
      "===============memory size:  970\n",
      "52 running reward: 14.160226 actual reward: 12.000000\n",
      "===============memory size:  982\n",
      "53 running reward: 14.252215 actual reward: 16.000000\n",
      "===============memory size:  998\n",
      "54 running reward: 14.239604 actual reward: 14.000000\n",
      "===============memory size:  1012\n",
      "55 running reward: 15.977624 actual reward: 49.000000\n",
      "===============memory size:  1061\n",
      "56 running reward: 17.578743 actual reward: 48.000000\n",
      "===============memory size:  1109\n",
      "57 running reward: 19.599806 actual reward: 58.000000\n",
      "===============memory size:  1167\n",
      "58 running reward: 21.519815 actual reward: 58.000000\n",
      "===============memory size:  1225\n",
      "59 running reward: 22.143825 actual reward: 34.000000\n",
      "===============memory size:  1259\n",
      "60 running reward: 25.236633 actual reward: 84.000000\n",
      "===============memory size:  1343\n",
      "61 running reward: 26.024802 actual reward: 41.000000\n",
      "===============memory size:  1384\n",
      "62 running reward: 26.973562 actual reward: 45.000000\n",
      "===============memory size:  1429\n",
      "63 running reward: 27.524884 actual reward: 38.000000\n",
      "===============memory size:  1467\n",
      "64 running reward: 29.498639 actual reward: 67.000000\n",
      "===============memory size:  1534\n",
      "65 running reward: 30.173707 actual reward: 43.000000\n",
      "===============memory size:  1577\n",
      "66 running reward: 30.415022 actual reward: 35.000000\n",
      "===============memory size:  1612\n",
      "67 running reward: 30.394271 actual reward: 30.000000\n",
      "===============memory size:  1642\n",
      "68 running reward: 34.924557 actual reward: 121.000000\n",
      "===============memory size:  1763\n",
      "69 running reward: 35.478330 actual reward: 46.000000\n",
      "===============memory size:  1809\n",
      "70 running reward: 37.654413 actual reward: 79.000000\n",
      "===============memory size:  1888\n",
      "71 running reward: 38.021692 actual reward: 45.000000\n",
      "===============memory size:  1933\n",
      "72 running reward: 40.070608 actual reward: 79.000000\n",
      "===============memory size:  2012\n",
      "73 running reward: 40.367077 actual reward: 46.000000\n",
      "===============memory size:  2058\n",
      "74 running reward: 40.398724 actual reward: 41.000000\n",
      "===============memory size:  2099\n",
      "75 running reward: 41.278787 actual reward: 58.000000\n",
      "===============memory size:  2157\n",
      "76 running reward: 43.214848 actual reward: 80.000000\n",
      "===============memory size:  2237\n",
      "77 running reward: 44.254106 actual reward: 64.000000\n",
      "===============memory size:  2301\n",
      "78 running reward: 47.641400 actual reward: 112.000000\n",
      "===============memory size:  2413\n",
      "79 running reward: 48.059330 actual reward: 56.000000\n",
      "===============memory size:  2469\n",
      "80 running reward: 49.106364 actual reward: 69.000000\n",
      "===============memory size:  2538\n",
      "81 running reward: 51.051046 actual reward: 88.000000\n",
      "===============memory size:  2626\n",
      "82 running reward: 53.198493 actual reward: 94.000000\n",
      "===============memory size:  2720\n",
      "83 running reward: 54.838569 actual reward: 86.000000\n",
      "===============memory size:  2806\n",
      "84 running reward: 55.196640 actual reward: 62.000000\n",
      "===============memory size:  2868\n",
      "85 running reward: 54.886808 actual reward: 49.000000\n",
      "===============memory size:  2917\n",
      "86 running reward: 54.692468 actual reward: 51.000000\n",
      "===============memory size:  2968\n",
      "87 running reward: 55.807844 actual reward: 77.000000\n",
      "===============memory size:  3045\n",
      "88 running reward: 56.667452 actual reward: 73.000000\n",
      "===============memory size:  3118\n",
      "89 running reward: 56.534080 actual reward: 54.000000\n",
      "===============memory size:  3172\n",
      "90 running reward: 55.957376 actual reward: 45.000000\n",
      "===============memory size:  3217\n",
      "91 running reward: 55.809507 actual reward: 53.000000\n",
      "===============memory size:  3270\n",
      "92 running reward: 59.569031 actual reward: 131.000000\n",
      "===============memory size:  3401\n",
      "93 running reward: 61.290580 actual reward: 94.000000\n",
      "===============memory size:  3495\n",
      "94 running reward: 62.526051 actual reward: 86.000000\n",
      "===============memory size:  3581\n",
      "95 running reward: 63.999748 actual reward: 92.000000\n",
      "===============memory size:  3673\n",
      "96 running reward: 64.349761 actual reward: 71.000000\n",
      "===============memory size:  3744\n",
      "97 running reward: 66.082273 actual reward: 99.000000\n",
      "===============memory size:  3843\n",
      "98 running reward: 65.978159 actual reward: 64.000000\n",
      "===============memory size:  3907\n",
      "99 running reward: 65.729251 actual reward: 61.000000\n",
      "===============memory size:  3968\n",
      "100 running reward: 65.992789 actual reward: 71.000000\n",
      "===============memory size:  4039\n",
      "101 running reward: 66.793149 actual reward: 82.000000\n",
      "===============memory size:  4121\n",
      "102 running reward: 68.703492 actual reward: 105.000000\n",
      "===============memory size:  4226\n",
      "103 running reward: 70.968317 actual reward: 114.000000\n",
      "===============memory size:  4340\n",
      "104 running reward: 70.769901 actual reward: 67.000000\n",
      "===============memory size:  4407\n",
      "105 running reward: 70.531406 actual reward: 66.000000\n",
      "===============memory size:  4473\n",
      "106 running reward: 70.204836 actual reward: 64.000000\n",
      "===============memory size:  4537\n",
      "107 running reward: 71.194594 actual reward: 90.000000\n",
      "===============memory size:  4627\n",
      "108 running reward: 70.734864 actual reward: 62.000000\n",
      "===============memory size:  4689\n",
      "109 running reward: 71.248121 actual reward: 81.000000\n",
      "===============memory size:  4770\n",
      "110 running reward: 70.885715 actual reward: 64.000000\n",
      "===============memory size:  4834\n",
      "111 running reward: 72.941429 actual reward: 112.000000\n",
      "===============memory size:  4946\n",
      "112 running reward: 72.644358 actual reward: 67.000000\n",
      "===============memory size:  5013\n",
      "113 running reward: 72.312140 actual reward: 66.000000\n",
      "===============memory size:  5079\n",
      "114 running reward: 74.296533 actual reward: 112.000000\n",
      "===============memory size:  5191\n",
      "115 running reward: 74.081706 actual reward: 70.000000\n",
      "===============memory size:  5261\n",
      "116 running reward: 74.277621 actual reward: 78.000000\n",
      "===============memory size:  5339\n",
      "117 running reward: 77.763740 actual reward: 144.000000\n",
      "===============memory size:  5483\n",
      "118 running reward: 78.225553 actual reward: 87.000000\n",
      "===============memory size:  5570\n",
      "119 running reward: 77.864275 actual reward: 71.000000\n",
      "===============memory size:  5641\n",
      "120 running reward: 76.571062 actual reward: 52.000000\n",
      "===============memory size:  5693\n",
      "121 running reward: 75.992509 actual reward: 65.000000\n",
      "===============memory size:  5758\n",
      "122 running reward: 78.192883 actual reward: 120.000000\n",
      "===============memory size:  5878\n",
      "123 running reward: 84.283239 actual reward: 200.000000\n",
      "124 running reward: 83.769077 actual reward: 74.000000\n",
      "===============memory size:  6152\n",
      "125 running reward: 82.480623 actual reward: 58.000000\n",
      "===============memory size:  6210\n",
      "126 running reward: 86.906592 actual reward: 171.000000\n",
      "===============memory size:  6381\n",
      "127 running reward: 86.961262 actual reward: 88.000000\n",
      "===============memory size:  6469\n",
      "128 running reward: 86.513199 actual reward: 78.000000\n",
      "===============memory size:  6547\n",
      "129 running reward: 84.937539 actual reward: 55.000000\n",
      "===============memory size:  6602\n",
      "130 running reward: 84.440662 actual reward: 75.000000\n",
      "===============memory size:  6677\n",
      "131 running reward: 83.118629 actual reward: 58.000000\n",
      "===============memory size:  6735\n",
      "132 running reward: 82.412698 actual reward: 69.000000\n",
      "===============memory size:  6804\n",
      "133 running reward: 82.592063 actual reward: 86.000000\n",
      "===============memory size:  6890\n",
      "134 running reward: 81.762460 actual reward: 66.000000\n",
      "===============memory size:  6956\n",
      "135 running reward: 82.874337 actual reward: 104.000000\n",
      "===============memory size:  7060\n",
      "136 running reward: 82.980620 actual reward: 85.000000\n",
      "===============memory size:  7145\n",
      "137 running reward: 85.581589 actual reward: 135.000000\n",
      "===============memory size:  7280\n",
      "138 running reward: 86.002509 actual reward: 94.000000\n",
      "===============memory size:  7374\n",
      "139 running reward: 84.402384 actual reward: 54.000000\n",
      "===============memory size:  7428\n",
      "140 running reward: 87.582265 actual reward: 148.000000\n",
      "===============memory size:  7576\n",
      "141 running reward: 86.503152 actual reward: 66.000000\n",
      "===============memory size:  7642\n",
      "142 running reward: 85.627994 actual reward: 69.000000\n",
      "===============memory size:  7711\n",
      "143 running reward: 88.696594 actual reward: 147.000000\n",
      "===============memory size:  7858\n",
      "144 running reward: 92.161765 actual reward: 158.000000\n",
      "===============memory size:  8016\n",
      "145 running reward: 95.503676 actual reward: 159.000000\n",
      "===============memory size:  8175\n",
      "146 running reward: 93.878493 actual reward: 63.000000\n",
      "===============memory size:  8238\n",
      "147 running reward: 95.984568 actual reward: 136.000000\n",
      "===============memory size:  8374\n",
      "148 running reward: 101.185339 actual reward: 200.000000\n",
      "149 running reward: 99.526073 actual reward: 68.000000\n",
      "===============memory size:  8642\n",
      "150 running reward: 100.599769 actual reward: 121.000000\n",
      "===============memory size:  8763\n",
      "151 running reward: 105.269780 actual reward: 194.000000\n",
      "===============memory size:  8957\n",
      "152 running reward: 107.006291 actual reward: 140.000000\n",
      "===============memory size:  9097\n",
      "153 running reward: 105.255977 actual reward: 72.000000\n",
      "===============memory size:  9169\n",
      "154 running reward: 104.193178 actual reward: 84.000000\n",
      "===============memory size:  9253\n",
      "155 running reward: 105.083519 actual reward: 122.000000\n",
      "===============memory size:  9375\n",
      "156 running reward: 105.029343 actual reward: 104.000000\n",
      "===============memory size:  9479\n",
      "157 running reward: 107.727876 actual reward: 159.000000\n",
      "===============memory size:  9638\n",
      "158 running reward: 111.741482 actual reward: 188.000000\n",
      "===============memory size:  9826\n",
      "159 running reward: 110.504408 actual reward: 87.000000\n",
      "===============memory size:  9913\n",
      "160 running reward: 111.479188 actual reward: 130.000000\n",
      "===============memory size:  10043\n",
      "161 running reward: 109.455228 actual reward: 71.000000\n",
      "===============memory size:  10114\n",
      "162 running reward: 107.582467 actual reward: 72.000000\n",
      "===============memory size:  10186\n",
      "163 running reward: 110.403344 actual reward: 164.000000\n",
      "===============memory size:  10350\n",
      "164 running reward: 110.833176 actual reward: 119.000000\n",
      "===============memory size:  10469\n",
      "165 running reward: 110.841518 actual reward: 111.000000\n",
      "===============memory size:  10580\n",
      "166 running reward: 113.949442 actual reward: 173.000000\n",
      "===============memory size:  10753\n",
      "167 running reward: 111.601970 actual reward: 67.000000\n",
      "===============memory size:  10820\n",
      "168 running reward: 112.771871 actual reward: 135.000000\n",
      "===============memory size:  10955\n",
      "169 running reward: 110.883278 actual reward: 75.000000\n",
      "===============memory size:  11030\n",
      "170 running reward: 113.089114 actual reward: 155.000000\n",
      "===============memory size:  11185\n",
      "171 running reward: 111.184658 actual reward: 75.000000\n",
      "===============memory size:  11260\n",
      "172 running reward: 108.675425 actual reward: 61.000000\n",
      "===============memory size:  11321\n",
      "173 running reward: 112.141654 actual reward: 178.000000\n",
      "===============memory size:  11499\n",
      "174 running reward: 111.884571 actual reward: 107.000000\n",
      "===============memory size:  11606\n",
      "175 running reward: 110.040343 actual reward: 75.000000\n",
      "===============memory size:  11681\n",
      "176 running reward: 109.138325 actual reward: 92.000000\n",
      "===============memory size:  11773\n",
      "177 running reward: 113.681409 actual reward: 200.000000\n",
      "178 running reward: 117.997339 actual reward: 200.000000\n",
      "179 running reward: 119.247472 actual reward: 143.000000\n",
      "===============memory size:  12316\n",
      "180 running reward: 116.835098 actual reward: 71.000000\n",
      "===============memory size:  12387\n",
      "181 running reward: 120.993343 actual reward: 200.000000\n",
      "182 running reward: 120.643676 actual reward: 114.000000\n",
      "===============memory size:  12701\n",
      "183 running reward: 119.661492 actual reward: 101.000000\n",
      "===============memory size:  12802\n",
      "184 running reward: 122.178418 actual reward: 170.000000\n",
      "===============memory size:  12972\n",
      "185 running reward: 122.269497 actual reward: 124.000000\n",
      "===============memory size:  13096\n",
      "186 running reward: 126.156022 actual reward: 200.000000\n",
      "187 running reward: 129.848221 actual reward: 200.000000\n",
      "188 running reward: 131.305810 actual reward: 159.000000\n",
      "===============memory size:  13655\n",
      "189 running reward: 132.040519 actual reward: 146.000000\n",
      "===============memory size:  13801\n",
      "190 running reward: 130.988493 actual reward: 111.000000\n",
      "===============memory size:  13912\n",
      "191 running reward: 131.189069 actual reward: 135.000000\n",
      "===============memory size:  14047\n",
      "192 running reward: 131.979615 actual reward: 147.000000\n",
      "===============memory size:  14194\n",
      "193 running reward: 129.330635 actual reward: 79.000000\n",
      "===============memory size:  14273\n",
      "194 running reward: 131.264103 actual reward: 168.000000\n",
      "===============memory size:  14441\n",
      "195 running reward: 130.700898 actual reward: 120.000000\n",
      "===============memory size:  14561\n",
      "196 running reward: 134.165853 actual reward: 200.000000\n",
      "197 running reward: 133.857560 actual reward: 128.000000\n",
      "===============memory size:  14889\n",
      "198 running reward: 137.164682 actual reward: 200.000000\n",
      "199 running reward: 140.306448 actual reward: 200.000000\n",
      "200 running reward: 143.291126 actual reward: 200.000000\n",
      "201 running reward: 146.126569 actual reward: 200.000000\n",
      "202 running reward: 148.820241 actual reward: 200.000000\n",
      "203 running reward: 151.379229 actual reward: 200.000000\n",
      "204 running reward: 150.910267 actual reward: 142.000000\n",
      "===============memory size:  16231\n",
      "205 running reward: 148.614754 actual reward: 105.000000\n",
      "===============memory size:  16336\n",
      "206 running reward: 151.184016 actual reward: 200.000000\n",
      "207 running reward: 148.924815 actual reward: 106.000000\n",
      "===============memory size:  16642\n",
      "208 running reward: 151.478575 actual reward: 200.000000\n",
      "209 running reward: 153.904646 actual reward: 200.000000\n",
      "210 running reward: 156.059414 actual reward: 197.000000\n",
      "===============memory size:  17239\n",
      "211 running reward: 152.606443 actual reward: 87.000000\n",
      "===============memory size:  17326\n",
      "212 running reward: 150.876121 actual reward: 118.000000\n",
      "===============memory size:  17444\n",
      "213 running reward: 153.332315 actual reward: 200.000000\n",
      "214 running reward: 154.865699 actual reward: 184.000000\n",
      "===============memory size:  17828\n",
      "215 running reward: 157.122414 actual reward: 200.000000\n",
      "216 running reward: 153.966293 actual reward: 94.000000\n",
      "===============memory size:  18122\n",
      "217 running reward: 154.717979 actual reward: 169.000000\n",
      "===============memory size:  18291\n",
      "218 running reward: 155.682080 actual reward: 174.000000\n",
      "===============memory size:  18465\n",
      "219 running reward: 152.797976 actual reward: 98.000000\n",
      "===============memory size:  18563\n",
      "220 running reward: 155.158077 actual reward: 200.000000\n",
      "221 running reward: 156.550173 actual reward: 183.000000\n",
      "===============memory size:  18946\n",
      "222 running reward: 156.472665 actual reward: 155.000000\n",
      "===============memory size:  19101\n",
      "223 running reward: 158.649031 actual reward: 200.000000\n",
      "224 running reward: 160.716580 actual reward: 200.000000\n",
      "225 running reward: 162.080751 actual reward: 188.000000\n",
      "===============memory size:  19689\n",
      "226 running reward: 163.626713 actual reward: 193.000000\n",
      "===============memory size:  19882\n",
      "227 running reward: 165.445378 actual reward: 200.000000\n",
      "228 running reward: 166.323109 actual reward: 183.000000\n",
      "===============memory size:  20265\n",
      "229 running reward: 168.006953 actual reward: 200.000000\n",
      "230 running reward: 167.456606 actual reward: 157.000000\n",
      "===============memory size:  20622\n",
      "231 running reward: 168.983775 actual reward: 198.000000\n",
      "===============memory size:  20820\n",
      "232 running reward: 170.534587 actual reward: 200.000000\n",
      "233 running reward: 172.007857 actual reward: 200.000000\n",
      "234 running reward: 173.407464 actual reward: 200.000000\n",
      "235 running reward: 172.887091 actual reward: 163.000000\n",
      "===============memory size:  21583\n",
      "236 running reward: 174.242737 actual reward: 200.000000\n",
      "237 running reward: 175.530600 actual reward: 200.000000\n",
      "238 running reward: 176.754070 actual reward: 200.000000\n",
      "239 running reward: 177.916366 actual reward: 200.000000\n",
      "240 running reward: 179.020548 actual reward: 200.000000\n",
      "241 running reward: 180.069521 actual reward: 200.000000\n",
      "242 running reward: 181.066045 actual reward: 200.000000\n",
      "243 running reward: 182.012742 actual reward: 200.000000\n",
      "244 running reward: 182.912105 actual reward: 200.000000\n",
      "245 running reward: 183.766500 actual reward: 200.000000\n",
      "246 running reward: 184.578175 actual reward: 200.000000\n",
      "247 running reward: 185.349266 actual reward: 200.000000\n",
      "248 running reward: 186.081803 actual reward: 200.000000\n",
      "249 running reward: 186.777713 actual reward: 200.000000\n",
      "250 running reward: 187.438827 actual reward: 200.000000\n",
      "251 running reward: 186.766886 actual reward: 174.000000\n",
      "===============memory size:  24757\n",
      "252 running reward: 187.428541 actual reward: 200.000000\n",
      "253 running reward: 186.907114 actual reward: 177.000000\n",
      "===============memory size:  25134\n",
      "254 running reward: 187.561759 actual reward: 200.000000\n",
      "255 running reward: 188.183671 actual reward: 200.000000\n",
      "256 running reward: 188.774487 actual reward: 200.000000\n",
      "257 running reward: 187.785763 actual reward: 169.000000\n",
      "===============memory size:  25903\n",
      "258 running reward: 188.396475 actual reward: 200.000000\n",
      "259 running reward: 188.976651 actual reward: 200.000000\n",
      "260 running reward: 188.877818 actual reward: 187.000000\n",
      "===============memory size:  26490\n",
      "261 running reward: 189.433927 actual reward: 200.000000\n",
      "262 running reward: 188.512231 actual reward: 171.000000\n",
      "===============memory size:  26861\n",
      "263 running reward: 189.086620 actual reward: 200.000000\n",
      "264 running reward: 189.632289 actual reward: 200.000000\n",
      "265 running reward: 190.150674 actual reward: 200.000000\n",
      "266 running reward: 190.643140 actual reward: 200.000000\n",
      "267 running reward: 191.110983 actual reward: 200.000000\n",
      "268 running reward: 191.555434 actual reward: 200.000000\n",
      "269 running reward: 191.977663 actual reward: 200.000000\n",
      "270 running reward: 192.378779 actual reward: 200.000000\n",
      "271 running reward: 192.759840 actual reward: 200.000000\n",
      "272 running reward: 193.121848 actual reward: 200.000000\n",
      "273 running reward: 193.465756 actual reward: 200.000000\n",
      "274 running reward: 193.792468 actual reward: 200.000000\n",
      "275 running reward: 194.102845 actual reward: 200.000000\n",
      "276 running reward: 194.397703 actual reward: 200.000000\n",
      "277 running reward: 194.677817 actual reward: 200.000000\n",
      "278 running reward: 194.943927 actual reward: 200.000000\n",
      "279 running reward: 195.196730 actual reward: 200.000000\n",
      "280 running reward: 195.436894 actual reward: 200.000000\n",
      "281 running reward: 195.665049 actual reward: 200.000000\n",
      "282 running reward: 195.881797 actual reward: 200.000000\n",
      "283 running reward: 196.087707 actual reward: 200.000000\n",
      "284 running reward: 196.283321 actual reward: 200.000000\n",
      "285 running reward: 196.469155 actual reward: 200.000000\n",
      "286 running reward: 196.645698 actual reward: 200.000000\n",
      "287 running reward: 196.813413 actual reward: 200.000000\n",
      "288 running reward: 196.972742 actual reward: 200.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5b44b4208374>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0msum_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-71bc8cc70f7c>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, observation, reward, done)\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[1;31m# 1. find k nearest neighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_pointer\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# L2 distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# sorts ascending by distance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[1;31m# ix = ix[:min(len(ix), self.nnfind)] # crop to only some number of nearest neighbors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\prerak - nonwork\\.conda\\envs\\py3.5\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36margsort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m     \"\"\"\n\u001b[0;32m--> 907\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'argsort'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mc:\\users\\prerak - nonwork\\.conda\\envs\\py3.5\\lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[1;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "#     logger = logging.getLogger()\n",
    "#     logger.setLevel(logging.INFO)\n",
    "\n",
    "    directory='training_dir'\n",
    "    env = gym.make('CartPole-v0')\n",
    "#     env = gym.wrappers.Monitor(env, directory)\n",
    "#     env.monitor.start('training_dir', force=True)\n",
    "    agent = EpisodicAgent(env.action_space)\n",
    "    \n",
    "\n",
    "    episode_count = 500\n",
    "    max_steps = 200\n",
    "    reward = 0\n",
    "    done = False\n",
    "    sum_reward_running = 0\n",
    "\n",
    "    for i in range(episode_count):\n",
    "        ob = env.reset()\n",
    "        sum_reward = 0\n",
    "\n",
    "        for j in range(max_steps):\n",
    "            action = agent.act(ob, reward, done)\n",
    "            ob, reward, done, _ = env.step(action)\n",
    "            sum_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        sum_reward_running = sum_reward_running * 0.95 + sum_reward * 0.05\n",
    "        print ('%d running reward: %f actual reward: %f' % (i, sum_reward_running,sum_reward))\n",
    "\n",
    "    # Dump monitor info to disk\n",
    "    env.monitor.close()\n",
    "    \n",
    "    # uncomment this line to also upload to OpenAI gym\n",
    "    #gym.upload('training_dir', algorithm_id='episodic_controller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'tensorflow' from 'c:\\\\users\\\\prerak - nonwork\\\\.conda\\\\envs\\\\py3.5\\\\lib\\\\site-packages\\\\tensorflow\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "print (tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
